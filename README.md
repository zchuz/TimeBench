# TimeBench
This is the repository containing evaluation datas scripts, associated with the forthcoming paper _TimeBench: Systematically Evaluating the Temporal Understanding Ability of Large Language Models" (tentative title)_

Temporal reasoning, as an essential component of comprehending the world, has frequently been overlooked in previous research on Large Language Models (LLMs). In order to draw attention from the research community towards the temporal understanding capability of LLMs, we have introduced a holistic temporal understanding benchmarks.

We proposed **TimeBench (Time Understanding Benchmark for Large Language Models)**,  which encompasses distinct sub-datasets for temporal reasoning. Concretely, it covers **fundamental temporal understanding** tasks such as date calculation and temporal entailment, as well as **common-sense temporal comprehension** tasks like duration prediction, temporal ordering, and commonsense generation. Furthermore, it addresses the **event temporal understanding**, including tasks involving temporal question answering and reasoning over time-sensitive passages.

Our evaluation systematically assesses the temporal understanding capabilities of Large Language Models (LLMs) across a spectrum of complexity, progressing from shallow comprehension to deep reasoning. We have employed a range of LLMs, including ChatGPT, GPT4, LLAMA2, Falcon, ChatGLM-3, Baichuan-2, Mistral, and FLAN-T5. The experimental results demonstrate that the LLM's temporal reasoning is not flawless and remains notably distant from human-level performance, which needs further research endeavors.



